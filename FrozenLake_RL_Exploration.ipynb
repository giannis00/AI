{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a6db699-fc04-4610-80ac-326e44230c7f",
   "metadata": {},
   "source": [
    "\n",
    "## Reinforcement Learning Lab: Navigating the Frozen Lake with OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a2753-e3a0-4d8d-9c68-f4242758c869",
   "metadata": {},
   "source": [
    "# Visual Example of the Frozen Lake Environment\n",
    "To get a sense of the environment, here's a visualization of the Frozen Lake game from Gymnasium, featuring the elf character navigating the grid.\n",
    "https://gymnasium.farama.org/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85856171-abbf-4b39-80c7-4ea9caf99a8b",
   "metadata": {},
   "source": [
    "## Introduction to Reinforcement Learning (RL)\n",
    "\n",
    "**Reinforcement Learning (RL)** is a type of machine learning where an **agent** learns to make decisions by interacting with an **environment**. The agent takes **actions**, receives **rewards** (positive or negative feedback), and aims to **maximize cumulative reward** over time.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **State (s)** | Current situation of the environment |\n",
    "| **Action (a)** | Choice made by the agent |\n",
    "| **Reward (r)** | Immediate feedback after an action |\n",
    "| **Policy (Ï€)** | Strategy that maps states to actions |\n",
    "| **Q-function (Q(s,a))** | Expected cumulative reward starting from state `s`, taking action `a`, and following policy `Ï€` |\n",
    "\n",
    "---\n",
    "\n",
    "### RL Environment: FrozenLake-v1 (Gymnasium)\n",
    "\n",
    "- **Description:** Navigate a frozen lake from **Start (S)** to **Goal (G)**, avoiding holes (**H**). The ice is frozen (**F**) but slippery by default.\n",
    "- **Grid Size:** 4x4 (16 states) or 8x8 (64 states). We'll use **4x4** for simplicity.\n",
    "- **States:** Discrete positions on the grid (0 to 15 for 4x4).\n",
    "- **Actions:**\n",
    "  - `0` = left\n",
    "  - `1` = down\n",
    "  - `2` = right\n",
    "  - `3` = up\n",
    "- **Rewards:** \n",
    "  - `+1` for reaching goal\n",
    "  - `0` otherwise\n",
    "  - Episode ends on **goal** or **hole**\n",
    "- **Slippery Mode:** Actions have 1/3 chance of slipping. Initially disabled for deterministic learning; later included as an exercise.\n",
    "- **Goal:** Learn a **policy** to reach `G` safely using **Q-Learning**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf68e4e-a330-4349-b1b9-688f650e8ce8",
   "metadata": {},
   "source": [
    "## Installation of Required Packages\n",
    "Before starting, we need to install the necessary libraries. Gymnasium provides the environment, and Pygame is required for graphical rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a339088-f882-40a7-af11-a0c0fe674438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\programdata\\anaconda3\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: pygame in c:\\programdata\\anaconda3\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (1.22.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\ioann\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ioann\\appdata\\roaming\\python\\python39\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium pygame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68309cbb-9ad4-4840-a13d-1d0587e3fa8c",
   "metadata": {},
   "source": [
    "## Imports and Environment Setup\n",
    "\n",
    "Before we start, we need to **import the necessary packages** and **set up the FrozenLake environment**.  \n",
    "We'll also set **random seeds** to ensure our results are **reproducible**.\n",
    "\n",
    "### Environment Details\n",
    "- **Grid:** 4x4 (16 states)  \n",
    "- **Slippery:** Disabled (`is_slippery=False`) for deterministic learning  \n",
    "- **Goal:** Make learning simpler for initial experiments  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8147efa4-d54c-430b-a025-9bbb1e9b9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221c993-527b-4db3-9e3d-79c96ad8515c",
   "metadata": {},
   "source": [
    "## Setting Up the FrozenLake Environment\n",
    "\n",
    "In this step, we create the **FrozenLake environment** using a **4x4 grid** and **disable slippery mode**. This ensures that the agent's actions are **deterministic**, making it easier to understand the learning process.\n",
    "\n",
    "We also check the **number of states and actions** to verify the environment's configuration.  \n",
    "This setup is important for understanding the **discrete state and action space** that Q-Learning will operate on.\n",
    "\n",
    "> âœ… Using a deterministic environment allows us to **focus on learning the Q-Learning algorithm** without additional randomness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31b257ef-c9c4-4b9f-bac8-b1849bb370d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 16\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    map_name=\"4x4\",\n",
    "    is_slippery=False\n",
    ")\n",
    "\n",
    "print(\"Number of states:\", env.observation_space.n)\n",
    "print(\"Number of actions:\", env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48344dd3-98de-4ba0-9ab0-3904a3c504a4",
   "metadata": {},
   "source": [
    "## Defining Action Mappings\n",
    "\n",
    "In this step, we define a **dictionary called `actions`** that maps the **numerical action indices** (`0` to `3`) from the FrozenLake environment to **human-readable strings**:  \n",
    "\n",
    "- `0` â†’ \"LEFT\"  \n",
    "- `1` â†’ \"DOWN\"  \n",
    "- `2` â†’ \"RIGHT\"  \n",
    "- `3` â†’ \"UP\"  \n",
    "\n",
    "This mapping makes it easier to **interpret and debug** the agent's actions during **rendering or evaluation**.  \n",
    "At the end, we can display the dictionary contents to **verify the mappings**.\n",
    "\n",
    "> âœ… This step ensures that when the agent moves, we can **quickly understand what each action means**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5477eff5-00ff-4a6a-be03-8240ba854260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'LEFT', 1: 'DOWN', 2: 'RIGHT', 3: 'UP'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = {\n",
    "    0: \"LEFT\",\n",
    "    1: \"DOWN\",\n",
    "    2: \"RIGHT\",\n",
    "    3: \"UP\"\n",
    "}\n",
    "\n",
    "actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fbb4f-0691-48de-94f2-14e9fd7c0d31",
   "metadata": {},
   "source": [
    "## Initializing the Q-Table\n",
    "\n",
    "In this step, we create the **Q-table** as a **2D NumPy array filled with zeros**.  \n",
    "\n",
    "- **Rows:** correspond to the **states** from the environment's observation space  \n",
    "- **Columns:** correspond to the **actions** from the action space  \n",
    "\n",
    "The Q-table will **store the expected rewards** for each **state-action pair**, which the agent will **update during Q-Learning training**.  \n",
    "\n",
    "Printing the Q-table at this stage allows us to **verify that it has been initialized correctly**.\n",
    "\n",
    "> âœ… At this point, the Q-table is empty, ready to be **populated as the agent learns**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "168ccee9-1f07-43ba-80df-c1e942846baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6808f7-589c-435f-ae62-ccb4ed836da4",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Action Selection Function\n",
    "\n",
    "In this step, we define the **`choose_action` function**, which implements an **epsilon-greedy policy** for Q-Learning.  \n",
    "\n",
    "- With probability **Îµ** (default 0.2), the agent **explores** by selecting a **random action**.  \n",
    "- Otherwise, the agent **exploits** its current knowledge by choosing the action with the **highest Q-value** for the given state.  \n",
    "\n",
    "This function is used during training to **decide the agent's action at each step**, balancing **exploration** of new actions and **exploitation** of learned rewards.\n",
    "\n",
    "> âœ… Epsilon-greedy ensures that the agent does not get stuck in suboptimal actions and continues to **learn from the environment**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2b7bfb2-ac7a-494b-8549-d08f6e1ecaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon=0.2):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()  # explore\n",
    "    else:\n",
    "        return np.argmax(Q[state])        # exploit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c619f3-7f5e-470f-a599-3ad1ad600373",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters for Q-Learning\n",
    "\n",
    "In this step, we set the **key hyperparameters** for the Q-Learning algorithm:\n",
    "\n",
    "- **Î± (alpha)** = 0.1 â†’ the **learning rate**, controlling how much Q-values are updated based on new information.  \n",
    "- **Î³ (gamma)** = 0.95 â†’ the **discount factor**, determining the importance of **future rewards** compared to immediate ones.  \n",
    "- **episodes** = 5000 â†’ the number of **training episodes**, specifying how many times the agent interacts with the environment to learn.  \n",
    "\n",
    "These hyperparameters can be **tuned** to improve convergence and performance:  \n",
    "- Lower **Î±** â†’ slower learning  \n",
    "- Higher **Î³** â†’ emphasizes **long-term rewards**  \n",
    "- More **episodes** â†’ allows for better **training and policy learning**\n",
    "\n",
    "> âœ… Proper hyperparameter tuning is essential for the agent to **learn an effective policy efficiently**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c02fb574-f953-43bf-9c80-3cc47b19103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1    # learning rate\n",
    "gamma = 0.95   # future importance\n",
    "episodes = 5000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a85064-d794-4412-b731-81cf2ce6a8c0",
   "metadata": {},
   "source": [
    "## Training the Q-Learning Agent\n",
    "\n",
    "In this step, we run the **core Q-Learning training loop** over the specified number of episodes (e.g., 5000).  \n",
    "\n",
    "### Training Process\n",
    "\n",
    "For each episode:\n",
    "\n",
    "1. **Reset the environment** to start a new episode and get the **initial state**.  \n",
    "2. While the episode is **not done** (the agent hasn't reached the goal or fallen into a hole):  \n",
    "   - **Choose an action** using the **epsilon-greedy policy** (`choose_action`).  \n",
    "   - **Step the environment** with the chosen action to obtain the **next state**, **reward**, and **termination flags**.  \n",
    "   - **Update the Q-table** for the current state-action pair using the **Q-Learning update rule**:\n",
    "\n",
    "   $$\n",
    "   Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\Big]\n",
    "   $$\n",
    "\n",
    "   Where:  \n",
    "   - $\\alpha$ = learning rate  \n",
    "   - $\\gamma$ = discount factor  \n",
    "3. **Set the current state** to the **next state**.\n",
    "\n",
    "### Outcome\n",
    "\n",
    "By iteratively updating the Q-table based on received rewards and expected future rewards, the agent **learns an optimal policy**.  \n",
    "After training, the **Q-table guides the agent** to make better decisions in the environment.\n",
    "\n",
    "> âœ… This loop is the core of Q-Learning, allowing the agent to **improve its behavior over time** through trial and error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40bdb7ca-7210-40fc-8229-2ffb395be39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        Q[state, action] += alpha * (\n",
    "            reward + gamma * np.max(Q[next_state]) - Q[state, action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc756ec-38c3-4411-9cc7-99e5270fbbb9",
   "metadata": {},
   "source": [
    "## Inspecting the Trained Q-Table\n",
    "\n",
    "After training, we can **examine the learned Q-table** to understand the agent's behavior.\n",
    "\n",
    "- Use **NumPy's `round` function** to display Q-values rounded to **2 decimal places** for better readability.  \n",
    "- Each **row** represents a **state** (0 to 15) and each **column** corresponds to an **action**:  \n",
    "  - `0` â†’ LEFT  \n",
    "  - `1` â†’ DOWN  \n",
    "  - `2` â†’ RIGHT  \n",
    "  - `3` â†’ UP  \n",
    "- **Higher values** in a row indicate the **preferred action(s)** for that state.  \n",
    "- **Non-zero values** reflect the agentâ€™s learned preferences to **reach the goal efficiently**.  \n",
    "\n",
    "> âœ… Inspecting the Q-table helps us **visualize what the agent has learned** and verify that the policy is improving toward the goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6dffdc84-f11d-43a1-b59a-222395af0b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74, 0.77, 0.7 , 0.74],\n",
       "       [0.74, 0.  , 0.14, 0.42],\n",
       "       [0.44, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.77, 0.81, 0.  , 0.74],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.9 , 0.  , 0.04],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.81, 0.  , 0.86, 0.77],\n",
       "       [0.81, 0.9 , 0.9 , 0.  ],\n",
       "       [0.86, 0.95, 0.  , 0.86],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.62, 0.95, 0.71],\n",
       "       [0.9 , 0.95, 1.  , 0.9 ],\n",
       "       [0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(Q, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb8972c-460f-4c8a-8533-38f4a3e94639",
   "metadata": {},
   "source": [
    "## Setting Up a Renderable FrozenLake Environment\n",
    "\n",
    "In this step, we create a **new instance** of the FrozenLake environment called `env_render` for **visual demonstration**.\n",
    "\n",
    "### Key Details\n",
    "\n",
    "- **Grid:** 4x4 map  \n",
    "- **Slippery Mode:** Disabled (`is_slippery=False`) for **deterministic movements**  \n",
    "- **Render Mode:** `\"human\"` â†’ enables **graphical rendering** in a popup window (using Pygame)  \n",
    "\n",
    "### Purpose\n",
    "\n",
    "This setup allows us to **visually observe the agent** as it moves on the ice grid, represented as an **elf character**.  \n",
    "It is particularly useful for **demonstrating the trained policy** in action.  \n",
    "\n",
    "> âœ… Unlike the previous environment (`env`), `env_render` is specifically configured for **human-visible rendering**, making it ideal for demonstrations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e744677-473c-44a5-a603-98de8c2db55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_render = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    map_name=\"4x4\",\n",
    "    is_slippery=False,\n",
    "    render_mode=\"human\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945ee064-4d93-444e-8ce8-0e085a7dd37a",
   "metadata": {},
   "source": [
    "## Running a Trained Episode with Graphical Rendering and Random Start\n",
    "\n",
    "In this step, we **demonstrate the trained agent's policy** by running a **single episode** in the **graphical rendering environment** (`env_render`).\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Reset the environment** to obtain the **initial state**.  \n",
    "2. **Starting state (state 0):**  \n",
    "   - Select a **random action** from `DOWN (1)`, `RIGHT (2)`, or `UP (3)`  \n",
    "   - Adds **variety** and prevents potential stuck scenarios  \n",
    "3. **Other states:**  \n",
    "   - Choose the **greedy action** (highest Q-value) from the **trained Q-table**  \n",
    "4. **Print** the current state and chosen action using the **`actions` dictionary** for readability  \n",
    "5. **Step the environment** to get the **next state**, **reward**, and **termination flags**  \n",
    "   - Render the agentâ€™s movement as an **elf** in a popup window  \n",
    "6. **Loop ends** when the agent reaches the **goal** or falls into a **hole** (`done = True`)  \n",
    "\n",
    "### Outcome\n",
    "\n",
    "- Visual inspection of the agent's path on the frozen lake grid  \n",
    "- Confirms that the **trained Q-table policy** guides the agent effectively toward the goal  \n",
    "\n",
    "> âœ… This method allows us to **see the agent in action** and understand its learned behavior in a human-readable, graphical format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0ba4ceb-b6b6-40f5-8a3a-5609bbb29452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0, Action: DOWN\n",
      "State: 4, Action: DOWN\n",
      "State: 8, Action: RIGHT\n",
      "State: 9, Action: RIGHT\n",
      "State: 10, Action: DOWN\n",
      "State: 14, Action: RIGHT\n"
     ]
    }
   ],
   "source": [
    "state, info = env_render.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # pick the best action but add small randomness if stuck\n",
    "    if state == 0:\n",
    "        action = random.choice([1,2,3])  # DOWN, RIGHT, UP\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "    \n",
    "    print(f\"State: {state}, Action: {actions[action]}\")\n",
    "    state, reward, terminated, truncated, info = env_render.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "env_render.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec297fe-d4c9-49be-ba87-f5ae9f2df6b8",
   "metadata": {},
   "source": [
    "## Try It Yourself: Experiment and Observe\n",
    "\n",
    "Now that the agent has been trained and we can run episodes with graphical rendering, itâ€™s time to **explore and experiment**! Here are some ideas:\n",
    "\n",
    "- **Change the starting state:**  \n",
    "  Try starting from different states instead of always using state `0`. How does it affect the path the agent takes?  \n",
    "\n",
    "- **Modify epsilon or hyperparameters:**  \n",
    "  Adjust **epsilon**, **alpha**, **gamma**, or **number of episodes**, retrain the Q-table, and observe how the agentâ€™s behavior changes.  \n",
    "  - Lower epsilon â†’ less exploration, agent may get stuck in suboptimal paths  \n",
    "  - Higher gamma â†’ agent values long-term rewards more  \n",
    "  - More episodes â†’ Q-values converge more accurately  \n",
    "\n",
    "- **Enable slippery mode:**  \n",
    "  Set `is_slippery=True` and see how the **randomness in actions** affects the learned policy. How does the agent adapt to uncertainty?  \n",
    "\n",
    "- **Inspect the Q-table:**  \n",
    "  Look at the rounded Q-values for different states. Which actions are preferred? How do changes in hyperparameters affect these preferences?  \n",
    "\n",
    "- **Visual observation:**  \n",
    "  Watch the agent move on the frozen lake. Try different paths and starting points. Can you predict its actions before they happen?  \n",
    "\n",
    "> ðŸ’¡ **Tip:** Experimenting and observing the outputs will help you **understand the effects of hyperparameters, randomness, and learning in Q-Learning**. Take notes on how the policy changes and what it tells you about the agent's decision-making process.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
